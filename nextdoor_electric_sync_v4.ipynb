{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reddph/Nextdoor-Project/blob/main/nextdoor_electric_sync_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KciYjAL7nb4x"
      },
      "source": [
        "# Power To Choose - Nextdoor group sync"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4HTTULrznZx6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you wanted to load csv file from a location in google drive, this would be how to do it, check the next cell,\n",
        "# I am loading the full csv realtime\n",
        "#from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "#latest_data_in_drive = '/content/drive/MyDrive/ME/nextdoor-app/ptc-rates-06-26-25.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "957134e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18870fa1-a7d2-4184-b694-97f0ebe3a75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching the URL: 404 Client Error: Not Found for url: https://www.powertochoose.org/en-us/Plan/ExportToCsv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import io\n",
        "\n",
        "latest_ptc_url = 'https://www.powertochoose.org/en-us/Plan/ExportToCsv'\n",
        "\n",
        "# Use requests to get the content from the URL\n",
        "# By default, requests verifies SSL certificates\n",
        "try:\n",
        "    response = requests.get(latest_ptc_url, verify=True) # Added verify=False\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    # Read the content into a pandas DataFrame\n",
        "    df0 = pd.read_csv(io.StringIO(response.text))\n",
        "\n",
        "    # Set option to display all columns\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    #display(df0.head())\n",
        "    #display(df0.info())\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching the URL: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "0eh_8rp7pTDn",
        "outputId": "40a52b88-8400-49a7-cb31-4a4f72fdd916"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df0' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3548092402.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#first strip out all nan values or empty cells for the [RepCompany] column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'[RepCompany]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df0' is not defined"
          ]
        }
      ],
      "source": [
        "#first strip out all nan values or empty cells for the [RepCompany] column\n",
        "df = df0[df0['[RepCompany]'].notna()]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb_0Qm3mn41C"
      },
      "outputs": [],
      "source": [
        "# Need to edit multiple columns that have values of FALSE & TRUE, filter only to include FALSE:\n",
        "# [PrePaid] [TimeOfUse] [MinUsageFeesCredits]\n",
        "df = df[df['[PrePaid]'] == False]\n",
        "df = df[df['[TimeOfUse]'] == False]\n",
        "df = df[df['[MinUsageFeesCredits]'] == False]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE-AiUNAn41T"
      },
      "outputs": [],
      "source": [
        "# Filter only to include English for [Language] (last column)\n",
        "# Filter only include ONCOR for [TduCompanyName] (second column)\n",
        "\n",
        "df = df[(df['[Language]'] == 'English') & (df['[TduCompanyName]'].str.contains('ONCOR'))]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSEnq-HTn41T"
      },
      "outputs": [],
      "source": [
        "#Exclude these companies: Tara, Amigos, Just\n",
        "df_excl = df[(df['[RepCompany]'].str.contains('TARA ENERGY')) | (df['[RepCompany]'].str.contains('AMIGOS ENERGY')) | (df['[RepCompany]'].str.contains('JUST ENERGY')) | (df['[RepCompany]'].str.contains('SPARK ENERGY LLC'))]\n",
        "\n",
        "#display(df_excl.head(20))\n",
        "\n",
        "df = df[df['[idKey]'].isin(df_excl['[idKey]']) == False]\n",
        "\n",
        "#df.head()\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jNLKxdtn41T"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Exclude plans that require Smart Thermostat\n",
        "df_smartTherm = df[(df['[Product]'].str.contains('Thermostat', case=False, na=False)) | (df['[SpecialTerms]'].str.contains('Thermostat', case=False, na=False))]\n",
        "#display(df_smartTherm.head(20))\n",
        "\n",
        "df = df[df['[idKey]'].isin(df_smartTherm['[idKey]']) == False]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0dkZSmIn41T"
      },
      "outputs": [],
      "source": [
        "# Criteria for excluding plans based on terms in [CancelFee] column in the table\n",
        "# Had an email discussion with Paul. Paul figured out the following logic for inclusion/exclusion of plans:\n",
        "# One observation is that about 60% of plans are within a $150 fee range.\n",
        "# A fee of $100 typically exceeds revenue of 1 month generated by \"energy charge\" under 8 cents at 1000 kWh.\n",
        "# Excessive cancellation fees are not in the best interest of the consumer.  Refine as follows:\n",
        "# If [CancelFee] is numeric and exceeds 150, exclude plan.\n",
        "# Else if [CancelFee] contains a numeric > 12, exclude plan.\n",
        "\n",
        "# Parsing pure numeric string to extract the monthly fee for left over months in the contract\n",
        "# Adding a temporary column CF1 for this scenario\n",
        "# Regex for pure numeric values is r'([0-9][0-9\\.]*$)'\n",
        "# Examples: 0, 350.00 or 100.00, etc.\n",
        "\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "\n",
        "df = df.assign(CF1 = lambda x: (x['[CancelFee]'].str.extract(r'([0-9][0-9\\.]*$)')))\n",
        "df.fillna({'CF1':-1}, inplace=True) # Placing an adhoc large per month fee value for nan on CF1\n",
        "df[[\"CF1\"]] = df[[\"CF1\"]].astype(float)\n",
        "\n",
        "# Parsing alphanumeric string to extract the monthly fee for left over months in the contract\n",
        "# Adding a temporary column CF2 for this scenario\n",
        "# Regex for pure numeric values is r'([1-9][0-9])[ /a-z]+' which is 2 digits of numeric followed by optional space, slash, followed by text string\n",
        "# Examples: 20/remaining month or 20 / remaining month, 20 /remaining month, etc.\n",
        "\n",
        "df = df.assign(CF2 = lambda x: (x['[CancelFee]'].str.extract(r'([1-9][0-9\\.]*)[ /a-z]+')))\n",
        "df.fillna({'CF2':-1}, inplace=True)\n",
        "df[[\"CF2\"]] = df[[\"CF2\"]].astype(float)\n",
        "\n",
        "#Exclude plans with CF1 > 150 or CF2 > 12\n",
        "df_cf_excl = df[(df['CF1'] >= 300) | (df['CF2'] >= 20)]\n",
        "\n",
        "df = df[df['[idKey]'].isin(df_cf_excl['[idKey]']) == False]\n",
        "\n",
        "# Sort ascending by [kwh1000]\n",
        "#df = df.sort_values(by=['[kwh1000]'])\n",
        "\n",
        "# Drop the temporaty columns CF1 and CF2 created for filtering on CancelFees terms\n",
        "df.drop(['CF1', 'CF2'], axis=1, inplace=True)\n",
        "\n",
        "#df.shape\n",
        "#df_cf_excl.head(40)\n",
        "#df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_sXjXHjn41T"
      },
      "outputs": [],
      "source": [
        "# Cast TermValue values to int as they were converted to decimal on import\n",
        "df['[TermValue]'] = df['[TermValue]'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qazynK40n41T"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-q3uQdUn41T"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'[FactsURL]': 'FactsURL'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSZo7YTvn41T"
      },
      "outputs": [],
      "source": [
        "factsURL = list(df.loc[:,'FactsURL'])\n",
        "factsURL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqucZQeun41T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruIpRG3Kn41T"
      },
      "outputs": [],
      "source": [
        "!pip install python-magic\n",
        "!pip install pdfplumber\n",
        "!pip install pypdf\n",
        "!pip install python-dateutil\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_SCqGFpBsXxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK7Z3FKTn41T"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import magic\n",
        "import pdfplumber\n",
        "from pypdf import PdfReader\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime\n",
        "\n",
        "def remove_items(test_list, item):\n",
        "    # remove the item for all its occurrences\n",
        "    c = test_list.count(item)\n",
        "    for i in range(c):\n",
        "        test_list.remove(item)\n",
        "    return test_list\n",
        "\n",
        "def efl_content_extractor(facts_url):\n",
        "    print(facts_url)\n",
        "\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "\n",
        "    # temporary file that will be overwritten each time\n",
        "    file_path = '/content/drive/MyDrive/Nextdoor-Project/staging/X_EFL.abc'\n",
        "\n",
        "    try:\n",
        "        # Add headers to mimic a browser to prevent access issues\n",
        "        response = requests.get(facts_url, verify=True, headers = headers )\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        with open(file_path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "\n",
        "        # printing the mime type of the file\n",
        "        file_type = magic.from_file(file_path, mime = True)\n",
        "        #print('magic: ',file_type)\n",
        "\n",
        "        if 'html' in file_type:  # html reader\n",
        "\n",
        "            # Step 1: Parse HTML content\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Tried html2text... found it has problems in handling UTF-8 characters in html\n",
        "            # BeautifulSoup handles much better\n",
        "            #html_text = html2text.html2text(response.text)\n",
        "\n",
        "            # Step 2: Extract text\n",
        "            html_text = soup.get_text()\n",
        "            return html_text\n",
        "\n",
        "        elif 'pdf' in file_type:  # pdf reader\n",
        "\n",
        "            # pymupdf is having troubles with opening the document in some cases.\n",
        "            # Replacing the read with pypdf package\n",
        "            # code to extract text from pdf\n",
        "            # doc = pymupdf.open(file_path)\n",
        "            # page = doc[0] # load the required page (0-based index)\n",
        "            # pdf_text = page.get_text() # extract plain text\n",
        "\n",
        "            # creating a pdf reader object\n",
        "            #reader = PdfReader(file_path)\n",
        "\n",
        "            # Open the PDF file\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                #if len(pdf.pages) > 0:\n",
        "                # Access the first page of the PDF\n",
        "                first_page = pdf.pages[0]\n",
        "\n",
        "                # Extract the text from the first page\n",
        "                pdf_text = first_page.extract_text()\n",
        "                return pdf_text\n",
        "        else:\n",
        "            print(\"Encountered a hitherto unexpected file type\", file_type)\n",
        "            return math.nan\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(\"Error fetching the URL: {e}\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ULXz7spfn41T"
      },
      "outputs": [],
      "source": [
        "df = df.assign(FactsText = df['FactsURL'].apply(efl_content_extractor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cafkvpdn41T"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnHLGZpun41T"
      },
      "outputs": [],
      "source": [
        "df['FactsText'] = df['FactsText'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6h1Safgn41T"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "\n",
        "def efl_pdf_table_search(facts_url, search_str):\n",
        "    print(facts_url)\n",
        "    print(search_str)\n",
        "\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "\n",
        "    # temporary file that will be overwritten each time\n",
        "    file_path = '/home/reddph/Downloads/temp/X_EFL.xyz'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url,verify=True, headers=headers)\n",
        "        print(url)\n",
        "\n",
        "        with open(file_path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "\n",
        "        file_type = magic.from_file(file_path, mime = True)\n",
        "        #print('magic: ',file_type)\n",
        "\n",
        "        # Open the PDF file\n",
        "        pdf = pdfplumber.open(file_path)\n",
        "\n",
        "        page = pdf.pages[0]\n",
        "\n",
        "        # Extract tables from the page\n",
        "        tables = page.extract_tables()\n",
        "\n",
        "        ec = ['','','']\n",
        "        # Iterate over each table rows to search for energy charge item\n",
        "        for i, table in enumerate(tables):\n",
        "            for row in table:\n",
        "                if search_str in row:\n",
        "                    ec = row\n",
        "                    break\n",
        "\n",
        "        #print(ec)\n",
        "\n",
        "        return ec\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the URL: {e}\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ety9pqzdn41T"
      },
      "outputs": [],
      "source": [
        "def efl_parse_base_fee(x):\n",
        "    #print(x)\n",
        "    m0 = re.search('Average',x)\n",
        "\n",
        "    if m0:\n",
        "        buf0 = x[m0.end():]\n",
        "        m1 = re.search('Base', buf0)\n",
        "        if m1:\n",
        "            buf1 = buf0[m1.end():]\n",
        "            m2 = re.search(r'(\\$\\s?([0-9]+[\\.0-9]*))',buf1)\n",
        "            if m2:\n",
        "                val_check = float(m2.group(2))\n",
        "                if val_check > 0.0 and val_check < 1.0:\n",
        "                    buf2 = buf1[m2.end():]\n",
        "                    m3 = re.search(r'\\$\\s?([0-9]+[\\.0-9]*)',buf2)\n",
        "                    if m3:\n",
        "                        val_check = float(m3.group(1))\n",
        "                        if val_check > 0.0 and val_check < 1.0:\n",
        "                            #print('\\n***********BASE CHARGE: 0\\n')\n",
        "                            return 0\n",
        "                        else:\n",
        "                            #print('\\n***********SECOND SEARCH PASSED...BASE CHARGE:', m3.group(1))\n",
        "                            return m3.group(1)\n",
        "                    else:\n",
        "                        #print('\\n*********** SECOND SEARCH FAILED...BASE CHARGE: 0')\n",
        "                        return 0\n",
        "                else:\n",
        "                    #print('***********BASE CHARGE:', m2.group(2))\n",
        "                    return m2.group(2)\n",
        "            else:\n",
        "                #print('*************Base not found')\n",
        "                return 0\n",
        "        else:\n",
        "            #print('***********Key term Base not found, returning 0')\n",
        "            return 0\n",
        "    else:\n",
        "        return math.nan;\n",
        "\n",
        "# Paul's Specs:\n",
        "# All EFLs start with the AVERAGE section. That appears to be a standard. The AVERAGE section always have 6 numeric. So skip the first 6 numerics after the word \"Average\".\n",
        "# After that, the energy charge will be the first numeric which is shown as cents, such as a cent sign, the word cents, or a value less than 1, greater than zero.\n",
        "# As mentioned before, when expressed as cents (>1), the range is 3.0 to 25.0 cents\n",
        "\n",
        "def efl_parse_energy_charge(x):\n",
        "    #print(x)\n",
        "    clean_txt = re.sub(r\",\",\"\",x)\n",
        "    # Replace embedded whitespaces in decimal strings\n",
        "    # example 14. 2\n",
        "    clean_txt = re.sub(r'(\\d{2}\\.) (\\d)',r'\\1\\2', clean_txt)\n",
        "\n",
        "    #print(clean_txt)\n",
        "\n",
        "    m0 = re.search('Average',clean_txt)\n",
        "    if m0:\n",
        "        buf1 = clean_txt[m0.end():]\n",
        "        regex_ec = re.compile(r'((\\$|\\$ |\\$  )?([\\.0-9\\-]+)(cents| cents|¢| ¢)?)')\n",
        "        regex_pref = re.compile(r'\\$')\n",
        "        regex_suff = re.compile(r'cent|¢')\n",
        "        numList = re.findall(regex_ec,buf1)\n",
        "        numList = remove_items(numList,('.','','.',''))\n",
        "        numList = remove_items(numList,('. ', '', '',''))\n",
        "        numList = remove_items(numList,('. ', '.', '', ''))\n",
        "        numList = remove_items(numList,('. ', '. ', '', ''))\n",
        "        numList = remove_items(numList,('-', '', '-', ''))\n",
        "        cleanList = numList\n",
        "        #print(cleanList)\n",
        "        ec_df = pd.DataFrame(cleanList, columns=['Expr', 'Prefix', 'Item','Suffix'])\n",
        "        #print(ec_df.iloc[:,[1,2,3]])\n",
        "        for i in range(len(cleanList)):\n",
        "            if i < 6:\n",
        "                continue\n",
        "            else:\n",
        "                if i >= 6:\n",
        "                    sitem = ec_df.iloc[i,2]\n",
        "                    sprefix = ec_df.iloc[i,1]\n",
        "                    ssuffix = ec_df.iloc[i,3]\n",
        "                    try:\n",
        "                        item = float(sitem)\n",
        "                        if ((item >= 3.0 and item <= 25.0 and (regex_suff.search(ssuffix) != None)) or (item > 0 and item < 1 and (regex_pref.search(sprefix) != None))):\n",
        "                            #print('\\n*************selected item:',item)\n",
        "                            return item\n",
        "                        else:\n",
        "                            continue\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "        return math.nan\n",
        "    else:\n",
        "        #print('\\n**********No match found for Average')\n",
        "        return math.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWakWX2tn41T"
      },
      "outputs": [],
      "source": [
        "def efl_parse_date_from_efl(efl_text):\n",
        "\n",
        "    patterns = [\n",
        "        r'(\\d{4})-(\\d{2})-(\\d{2})',  # YYYY-MM-DD\n",
        "        r'(\\d{1,2})/(\\d{1,2})/(\\d{4})',  # MM/DD/YYYY\n",
        "        r'(\\d{2})-([A-Za-z]{3})-(\\d{4})',  # DD-Mon-YYYY\n",
        "        r'([A-Za-z]+) (\\d{1,2},) (\\d{4})',  # Month day, Year\n",
        "        r'(\\d{2})-(\\d{2})-(\\d{4})',  # MM-DD-YYYY\n",
        "        r'(\\d)/(\\d{1,2})/(\\d{2,4})'  # M/DD/YY\n",
        "    ]\n",
        "\n",
        "    dt_str = datetime.strptime('1900-01-01',\"%Y-%m-%d\")\n",
        "\n",
        "    clean_txt = re.sub(r'(\\d{4}-\\d) (\\d-\\d{2})',r'\\1\\2',efl_text)\n",
        "\n",
        "    for pattern in patterns:\n",
        "        #print(pattern)\n",
        "        matches = re.finditer(pattern,clean_txt)\n",
        "\n",
        "        for match in matches:\n",
        "            mgroup = match.group()\n",
        "            try:\n",
        "                if pattern == patterns[0]:\n",
        "                    dt_str = max(dt_str,datetime.strptime(mgroup, \"%Y-%m-%d\"))\n",
        "                elif pattern == patterns[1]:\n",
        "                    dt_str = max(dt_str,datetime.strptime(mgroup, \"%m/%d/%Y\"))\n",
        "                elif pattern == patterns[2]:\n",
        "                    dt_str = max(dt_str,datetime.strptime(mgroup, \"%d-%b-%Y\"))\n",
        "                elif pattern == patterns[3]:\n",
        "                    dt_str = max(dt_str,datetime.strptime(mgroup, \"%B %d, %Y\"))\n",
        "                elif pattern == patterns[4]:\n",
        "                    dt_str = max(dt_str,datetime.strptime(mgroup, \"%m-%d-%Y\"))\n",
        "                else:\n",
        "                    dt_str = max(dt_str,datetime.strptime(mgroup, \"%m/%d/%Y\"))\n",
        "            except ValueError:\n",
        "                print(\"error in date string\")\n",
        "\n",
        "    #if (dt_str == datetime.strptime('1900-01-01',\"%Y-%m-%d\")):\n",
        "    #    print(efl_text)\n",
        "\n",
        "    return(dt_str.strftime(\"%Y-%m-%d\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpoB8krtn41T"
      },
      "outputs": [],
      "source": [
        "df = df.assign(BaseFee = df['FactsText'].apply(efl_parse_base_fee))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFUjzSbsn41T"
      },
      "outputs": [],
      "source": [
        "df = df.assign(EnergyCharge = df['FactsText'].apply(efl_parse_energy_charge))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYSSoOrfn41T"
      },
      "outputs": [],
      "source": [
        "df = df.assign(EffectiveDate = df['FactsText'].apply(efl_parse_date_from_efl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAjdJt0An41T"
      },
      "outputs": [],
      "source": [
        "df.loc[:,['[RepCompany]','[TermValue]','[CancelFee]','BaseFee','EnergyCharge','EffectiveDate','FactsURL']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rffBsIQn41T"
      },
      "outputs": [],
      "source": [
        "df.drop('FactsText', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Heo6BqXFn41T"
      },
      "outputs": [],
      "source": [
        "# Rename the computed columns:\n",
        "df.rename(columns={\"BaseFee\": \"BASE_FEE\", \"EnergyCharge\": \"ENERGY_CHARGE\", \"EffectiveDate\":\"PUBLISHED_DATE\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwM1ik7ln41T"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jhEb0gEn41T"
      },
      "outputs": [],
      "source": [
        "df[['ENERGY_CHARGE']] = df[['ENERGY_CHARGE']].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v-0TswNn41T"
      },
      "outputs": [],
      "source": [
        "#  If EC < 1 cent, EC=EC*100   (Case: SoFed & RANCHERO )\n",
        "df['ENERGY_CHARGE'] = np.where(df['ENERGY_CHARGE'] < 1.0, df['ENERGY_CHARGE'] * 100, df['ENERGY_CHARGE'])\n",
        "\n",
        "# For all EC, display with 3 decimal places (N.NNN)\n",
        "#df['ENERGY_CHARGE'] = df['ENERGY_CHARGE'].round(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusted EC (@1000 kwh) = EC + base/1000\n",
        "df = df.assign(Adjusted_EC = df['ENERGY_CHARGE'] + df['BASE_FEE']/1000.0)"
      ],
      "metadata": {
        "id": "f-1iJIALbhuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR2LMiWRn41T"
      },
      "outputs": [],
      "source": [
        "df.loc[:,['[RepCompany]','[TermValue]','[CancelFee]','BASE_FEE','ENERGY_CHARGE','Adjusted_EC','PUBLISHED_DATE']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j09gkGlrn41T"
      },
      "outputs": [],
      "source": [
        "# Create separate groupings by term values: 12-month, 3-month, 30-day (variable rate), other\n",
        "df_12m = df[df['[TermValue]']==12].sort_values(by='Adjusted_EC',ascending=True).head(7)\n",
        "df_3m = df[df['[TermValue]']==3].sort_values(by='Adjusted_EC',ascending=True).head(7)\n",
        "df_4m = df[df['[TermValue]']==4].sort_values(by='Adjusted_EC',ascending=True).head(7)\n",
        "df_1m = df[df['[TermValue]']==1].sort_values(by='Adjusted_EC',ascending=True).head(7)\n",
        "# Group all other terms into other dataframe\n",
        "excl_terms = [1,3,4,12]\n",
        "df_other = df[~df['[TermValue]'].isin(excl_terms)].sort_values(by='Adjusted_EC',ascending=True).head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX_57UgBn41T"
      },
      "outputs": [],
      "source": [
        "df_1m"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/US/Central /etc/localtime\n",
        "!date"
      ],
      "metadata": {
        "id": "8fxlJDFUAIsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "local_tz = pytz.timezone('America/Chicago')"
      ],
      "metadata": {
        "id": "gN8GJlOsyN1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeCBIZIun41T"
      },
      "outputs": [],
      "source": [
        "def append_timestamp_to_name(prefix,suffix):\n",
        "    current_datetime = datetime.now(local_tz).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "    # convert datetime obj to string\n",
        "    str_current_datetime = str(current_datetime)\n",
        "\n",
        "    return prefix + \"__\" + str_current_datetime + '.' + suffix\n",
        "\n",
        "def generate_filename_by_term(folder,fname,term):\n",
        "    return folder + '/' + fname + '_' + term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btitQUuAn41T"
      },
      "outputs": [],
      "source": [
        "#df_12m['ENERGY_CHARGE'] = df_12m['ENERGY_CHARGE'].astype(str).map('{:.3f}'.format\n",
        "myfolder = '/content/drive/MyDrive/Nextdoor-Project/export'\n",
        "myprefix = 'PriceSheet'\n",
        "\n",
        "myterm = '12m'\n",
        "mypath = append_timestamp_to_name(generate_filename_by_term(myfolder,myprefix,myterm),'csv')\n",
        "df_12m.to_csv(mypath, sep='\\t', index=False, encoding='utf-8')\n",
        "\n",
        "myterm = '3m'\n",
        "mypath = append_timestamp_to_name(generate_filename_by_term(myfolder,myprefix,myterm),'csv')\n",
        "df_3m.to_csv(mypath, sep='\\t', index=False, encoding='utf-8')\n",
        "\n",
        "myterm = '4m'\n",
        "mypath = append_timestamp_to_name(generate_filename_by_term(myfolder,myprefix,myterm),'csv')\n",
        "df_4m.to_csv(mypath, sep='\\t', index=False, encoding='utf-8')\n",
        "\n",
        "myterm = '1m'\n",
        "mypath = append_timestamp_to_name(generate_filename_by_term(myfolder,myprefix,myterm),'csv')\n",
        "df_1m.to_csv(mypath, sep='\\t',index=False, encoding='utf-8')\n",
        "\n",
        "myterm = 'other'\n",
        "mypath = append_timestamp_to_name(generate_filename_by_term(myfolder,myprefix,myterm),'csv')\n",
        "df_other.to_csv(mypath, sep='\\t',index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FrFr9cln41T"
      },
      "outputs": [],
      "source": [
        "df_other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcVhMX25n41T"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDON18F_n41T"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}